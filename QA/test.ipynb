{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATION_DIR = '/root/Dataset/COSMOS/cosmos_anns_acm/acm_anns/'\n",
    "CONTEXT_DIR = '/root/Dataset/COSMOS/context/Context_EL/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from QA import answer\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from os.path import join, isfile\n",
    "from os import listdir\n",
    "from itertools import groupby\n",
    "import re\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_id(filename: str) -> int:\n",
    "    return int(re.search('([0-9]+)_[0-9]+\\.txt', filename).group(1))\n",
    "\n",
    "context_files = [join(CONTEXT_DIR, file) for file in listdir(CONTEXT_DIR) if isfile(join(CONTEXT_DIR, file))]\n",
    "context_files = sorted(context_files, key=get_image_id)\n",
    "\n",
    "context_files_grouped_by_image_id = {key: list(val) for key, val in groupby(context_files, key = get_image_id)}\n",
    "\n",
    "test_data = pd.read_json(join(ANNOTATION_DIR, 'test_data.json'), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['context_filename'] = pd.DataFrame([context_files_grouped_by_image_id]).T\n",
    "test_data['context_filename'] = [ [] if x is np.NaN else x for x in test_data['context_filename'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    context_filename = row.context_filename\n",
    "    answers = []\n",
    "    for filename in context_filename:\n",
    "        with open(filename, 'r', encoding='utf8') as ifile:\n",
    "            content = json.load(ifile)\n",
    "        context = content['context']\n",
    "        if context == '':\n",
    "            continue\n",
    "        context = ' '.join(context.split('\\n'))\n",
    "        answers.append(answer(context, row.caption1, row.caption2))\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/root/miniconda3/envs/NLP/lib/python3.10/site-packages/transformers/pipelines/base.py:1045: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 1000/1000 [02:09<00:00,  7.74it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data['answers'] = test_data.progress_apply(process, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('./df_answer_EL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "848ec3303aa6c64d107256817a16d6a039fa8cfd0fa250d143bf87759298d876"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
